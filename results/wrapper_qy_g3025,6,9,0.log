+ export CUDA_DEVICE_MAX_CONNECTIONS=1
+ CUDA_DEVICE_MAX_CONNECTIONS=1
+ srun -p arch -N 4 --ntasks-per-node=8 --gres=gpu:8 --mem 256G -K ./scripts/bench_ring_attn.sh python bench_ring_attn.py
srun: job 628 queued and waiting for resources
srun: job 628 has been allocated resources
torch distributed is already initialized, skipping initialization ...
************ Finish sequence pralell group Initialization. ***********
S=16384
# zigzag_ring_flash_attn_func, fwd
mfu: 1.609 Tflops/s, hfu: 1.609 Tflops/s, 23.408 iter/s, 4.272 sec
# zigzag_ring_flash_attn_func_opt, fwd
mfu: 0.888 Tflops/s, hfu: 0.888 Tflops/s, 12.928 iter/s, 7.735 sec
# hierarchy_attn_func, fwd
mfu: 0.915 Tflops/s, hfu: 0.915 Tflops/s, 13.321 iter/s, 7.507 sec
S=131072
# zigzag_ring_flash_attn_func, fwd
mfu: 25.769 Tflops/s, hfu: 25.769 Tflops/s, 5.859 iter/s, 17.067 sec
# zigzag_ring_flash_attn_func_opt, fwd
mfu: 43.426 Tflops/s, hfu: 43.426 Tflops/s, 9.874 iter/s, 10.128 sec
# hierarchy_attn_func, fwd
mfu: 18.041 Tflops/s, hfu: 18.041 Tflops/s, 4.102 iter/s, 24.377 sec
+ set +x
