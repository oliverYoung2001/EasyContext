+ export CUDA_DEVICE_MAX_CONNECTIONS=1
+ CUDA_DEVICE_MAX_CONNECTIONS=1
+ srun -p rag -N 1 --ntasks-per-node=4 --gres=gpu:4 --mem 256G -K -w g3017 -c 16 ./scripts/bench_ring_attn.sh python bench_ring_attn.py
srun: job 8070 queued and waiting for resources
srun: job 8070 has been allocated resources
torch distributed is already initialized, skipping initialization ...
************ Finish sequence pralell group Initialization. ***********
Nh=32, S=32768
# orchestrated_attn_func, fwd
rank0:
(0, 0, 0, 0, 0)
(0, 0, 1, 0, 0)
(0, 0, 2, 0, 0)
(0, 0, 1, 0, 1, 'o', 'r')   # send, 0 -> 1
(0, 0, 3, 0, 0)
(0, 0, 2, 0, 2, 'o', 'r')   # send, 0 -> 2
(0, 0, 3, 0, 3, 'o', 'r')   # send, 0 -> 3
rank1:
(0, 0, 1, 1, 1)
(0, 0, 3, 1, 1)
(0, 0, 3, 1, 3, 'o', 'r')   # send, 1 -> 3
(0, 0, 1, 0, 1, 'o', 'r')   # recv, 0 -> 1
(0, 0, 2, 1, 1)
(0, 0, 2, 1, 2, 'o', 'r')   # send, 1 -> 2
rank2:
(0, 0, 2, 2, 2)
(0, 0, 3, 2, 2)
(0, 0, 3, 2, 3, 'o', 'r')   # send, 2 -> 3
(0, 0, 2, 0, 2, 'o', 'r')   # recv, 0 -> 2
(0, 0, 2, 1, 2, 'o', 'r')   # recv, 1 -> 2
rank3:
(0, 0, 3, 3, 3)
(0, 0, 3, 1, 3, 'o', 'r')   # recv, 1 -> 3
(0, 0, 3, 2, 3, 'o', 'r')   # recv, 2 -> 3
(0, 0, 3, 0, 3, 'o', 'r')   # recv, 0 -> 3
rank1, Out !!!
rank1, cpu out !!!
rank2, Out !!!
rank1, sync out !!!
rank2, cpu out !!!
rank2, sync out !!!
rank3, Out !!!
rank3, cpu out !!!
rank0, Out !!!
rank3, sync out !!!
rank0, cpu out !!!
rank0, sync out !!!
rank1, real out !!!
rank2, real out !!!
rank3, real out !!!
rank0, real out !!!
rank1, Out !!!
rank1, cpu out !!!
rank3, Out !!!
rank3, cpu out !!!
rank2, Out !!!
rank2, cpu out !!!
rank0, Out !!!
rank0, cpu out !!!
rank3, sync out !!!
