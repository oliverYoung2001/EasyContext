+ export CUDA_DEVICE_MAX_CONNECTIONS=1
+ CUDA_DEVICE_MAX_CONNECTIONS=1
+ srun -p arch -N 4 --ntasks-per-node=8 --gres=gpu:8 --mem 256G -K ./scripts/bench_ring_attn.sh python bench_ring_attn.py
srun: job 629 queued and waiting for resources
srun: job 629 has been allocated resources
torch distributed is already initialized, skipping initialization ...
************ Finish sequence pralell group Initialization. ***********
S=16384
# zigzag_ring_flash_attn_func, fwd
mfu: 0.299 Tflops/s, hfu: 0.299 Tflops/s, 4.347 iter/s, 23.004 sec
# zigzag_ring_flash_attn_func_opt, fwd
mfu: 0.665 Tflops/s, hfu: 0.665 Tflops/s, 9.676 iter/s, 10.334 sec
# hierarchy_attn_func, fwd
mfu: 0.918 Tflops/s, hfu: 0.918 Tflops/s, 13.355 iter/s, 7.488 sec
S=131072
# zigzag_ring_flash_attn_func, fwd
mfu: 32.794 Tflops/s, hfu: 32.794 Tflops/s, 7.456 iter/s, 13.411 sec
# zigzag_ring_flash_attn_func_opt, fwd
mfu: 47.352 Tflops/s, hfu: 47.352 Tflops/s, 10.767 iter/s, 9.288 sec
# hierarchy_attn_func, fwd
mfu: 18.613 Tflops/s, hfu: 18.613 Tflops/s, 4.232 iter/s, 23.629 sec
+ set +x
