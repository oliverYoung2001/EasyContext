+ export CUDA_LAUNCH_BLOCKING=1
+ CUDA_LAUNCH_BLOCKING=1
+ export CUDA_DEVICE_MAX_CONNECTIONS=1
+ CUDA_DEVICE_MAX_CONNECTIONS=1
+ srun -p rag -N 1 --ntasks-per-node=4 --gres=gpu:4 --mem 256G -K -w g3017 -c 16 ./scripts/bench_ring_attn.sh python bench_ring_attn.py
srun: job 8118 queued and waiting for resources
srun: job 8118 has been allocated resources
torch distributed is already initialized, skipping initialization ...
************ Finish sequence pralell group Initialization. ***********
Nh=32, S=65536
# orchestrated_attn_func, fwd
rank0:
(0, 0, 0, 0, 0)
(0, 0, 1, 0, 0)
(0, 0, 2, 0, 0)
(0, 0, 1, 0, 1, 'o', 'r')
(0, 0, 3, 0, 0)
(0, 0, 2, 0, 2, 'o', 'r')
(0, 0, 3, 0, 3, 'o', 'r')
rank1:
(0, 0, 1, 1, 1)
(0, 0, 3, 1, 1)
(0, 0, 3, 1, 3, 'o', 'r')
(0, 0, 1, 0, 1, 'o', 'r')
(0, 0, 2, 1, 1)
(0, 0, 2, 1, 2, 'o', 'r')
rank2:
(0, 0, 2, 2, 2)
(0, 0, 3, 2, 2)
(0, 0, 3, 2, 3, 'o', 'r')
(0, 0, 2, 0, 2, 'o', 'r')
(0, 0, 2, 1, 2, 'o', 'r')
rank3:
(0, 0, 3, 3, 3)
(0, 0, 3, 1, 3, 'o', 'r')
(0, 0, 3, 2, 3, 'o', 'r')
(0, 0, 3, 0, 3, 'o', 'r')
rank3, (0, 0, 3, 3, 3) In !!!
rank2, (0, 0, 2, 2, 2) In !!!
rank1, (0, 0, 1, 1, 1) In !!!
rank0, (0, 0, 0, 0, 0) In !!!
rank3, (0, 0, 3, 3, 3) Out !!!
rank3, (0, 0, 3, 1, 3, 'o', 'r') In !!!
rank3, recv In !!!
rank2, (0, 0, 2, 2, 2) Out !!!
rank2, (0, 0, 3, 2, 2) In !!!
rank1, (0, 0, 1, 1, 1) Out !!!
rank1, (0, 0, 3, 1, 1) In !!!
rank0, (0, 0, 0, 0, 0) Out !!!
rank0, (0, 0, 1, 0, 0) In !!!
rank2, (0, 0, 3, 2, 2) Out !!!
rank2, (0, 0, 3, 2, 3, 'o', 'r') In !!!
rank2, send In !!!
rank1, (0, 0, 3, 1, 1) Out !!!
rank1, (0, 0, 3, 1, 3, 'o', 'r') In !!!
rank1, send In !!!
rank0, (0, 0, 1, 0, 0) Out !!!
rank0, (0, 0, 2, 0, 0) In !!!
rank0, (0, 0, 2, 0, 0) Out !!!
rank0, (0, 0, 1, 0, 1, 'o', 'r') In !!!
rank0, send In !!!
rank3, recv Out !!!
rank3, (0, 0, 3, 1, 3, 'o', 'r') Out !!!
rank3, (0, 0, 3, 2, 3, 'o', 'r') In !!!
rank3, recv In !!!
rank3, recv Out !!!
rank3, (0, 0, 3, 2, 3, 'o', 'r') Out !!!
rank3, (0, 0, 3, 0, 3, 'o', 'r') In !!!
rank3, recv In !!!
